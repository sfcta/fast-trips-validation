{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# format sig figs\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = r'J:\\Projects\\FasTrips\\obs\\output\\OBS_fasttrips_demand_v1.3_stochastic_iter1_nocap_1000'\n",
    "\n",
    "obs_links_dir = r'R:\\FastTrips\\PathChoiceValidation\\All input & output files\\OBS_FToutput.csv'\n",
    "# obs_links_dir = r'..\\data\\obs\\obs_links.csv'\n",
    "\n",
    "# Probability threshold for observed paths (are observed paths assigned above/below this value by fast-trips)\n",
    "threshold = 0.3\n",
    "\n",
    "# Compare observed path to pathset based on modes, agency, or route\n",
    "# comparison_field = 'path_modes'\n",
    "comparison_field = 'path_agencies'\n",
    "# comparison_field = 'path_routes'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network data - currently checked into this repo (1.9)\n",
    "taz = pd.read_csv(r'..\\data\\taz_coords.txt')\n",
    "walk_access_ft = pd.read_csv(r'..\\data\\walk_access_ft.txt')[['taz','stop_id','dist','elevation_gain', 'indirectness']]\n",
    "stops = pd.read_csv(r'..\\data\\stops.txt')\n",
    "\n",
    "non_transit_modes = ['transfer','walk_access','walk_egress','bike_access','bike_egress',\n",
    "                     'PNR_access','PNR_egress','KNR_access','KNR_egress']\n",
    "transit_mode_list = ['local_bus','commuter_rail','express_bus','ferry','heavy_rail','light_rail','premium_bus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_df(data, unique_fields, record_type=None):\n",
    "    '''Load text data as df, create unique trip record ID, and tag as model/observed record'''\n",
    "    df = pd.read_csv(data)\n",
    "    if record_type != None:\n",
    "        df['record_type'] = record_type    # tag as model/observed record\n",
    "\n",
    "    # Convert all specified unique_fields to string and concatenate as new unique_id field \n",
    "#     df[unique_fields] = pd.DataFrame([df[col].astype('int').astype('str') for col in unique_fields]).T\n",
    "#     df['unique_id'] = df[unique_fields].apply(lambda x: '_'.join(x), axis=1)\n",
    "    df['unique_id'] = df['person_id']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append(*args):\n",
    "    '''Union dataframes with similar structures'''\n",
    "    df = pd.DataFrame()\n",
    "    for data in args:\n",
    "        df = df.append(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    miles = 3956 * c\n",
    "    return miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_common_records(df1,df2,field):\n",
    "    '''Return dataframe of matching, common records only.\n",
    "       Example, person 1034 exists in df1, but not in df2, so new copy of df1 without 1034 is created\n",
    "    '''\n",
    "    df1 = df1[df1[field].isin(df2[field])]\n",
    "    df2 = df2[df2[field].isin(df1[field])]\n",
    "\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_transit_agency(df, routes):\n",
    "\n",
    "    df = pd.merge(left=df,right=routes[['route_id','agency_id']],on='route_id',how='left')\n",
    "\n",
    "    df['agency'] = df['agency_id']\n",
    "    df.drop('agency_id',axis=1)\n",
    "    df.fillna(\"\",inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def produce_path_fields(df, group):\n",
    "    '''\n",
    "    Concatenate set of fields for pathset_links, e.g. ('bart caltrain') for 2-leg transit trip\n",
    "    Produce concatenated fields for routes, modes, agencies, all components (stops, modes, & routes)\n",
    "    '''\n",
    "    # create \"path_routes\"\n",
    "\n",
    "    for field in ['route_id','mode','agency','A_id','B_id']:\n",
    "        df[field] = df[field].astype('str')\n",
    "        df[field] = df[field].fillna(\"\")\n",
    "        df[field] = df[field].replace('nan',\"\")\n",
    "\n",
    "    df['path_routes'] = df['route_id'].apply(lambda x: x.strip())\n",
    "    path_routes = pd.DataFrame(df.groupby(group)['path_routes'].apply(lambda x: \"%s\" % ' '.join(x).strip()))\n",
    "    \n",
    "    result_df = pd.DataFrame(index=path_routes.index)\n",
    "    result_df['path_routes'] = path_routes\n",
    "    \n",
    "    # create \"path_modes\"\n",
    "    df['path_modes'] = df['mode'].apply(lambda x: x.strip())\n",
    "    result_df['path_modes'] = pd.DataFrame(df.groupby(group)['mode'].apply(lambda x: \"%s\" % ' '.join(x).strip()))\n",
    "    \n",
    "    # create \"path_agencies\"\n",
    "    df['path_agencies'] = df['agency'].apply(lambda x: x.strip())\n",
    "    result_df['path_agencies'] = pd.DataFrame(df.groupby(group)['agency'].apply(lambda x: \"%s\" % ' '.join(x).strip()))\n",
    "\n",
    "    # Create \"path_components\"\n",
    "    df['path_components'] = df['A_id']+\" \"+df['mode']+\" \"+df['route_id'] +\"_\"+ df['B_id']\n",
    "    df['path_components'] = df['path_components'].apply(lambda x: x.strip())\n",
    "    result_df['path_components'] = pd.DataFrame(df.groupby(group)['path_components'].apply(lambda x: \"%s\" % ' '.join(x).strip()))\n",
    "    \n",
    "    # stop components\n",
    "    df['A_id'] = df['A_id'].astype('str')\n",
    "    df['B_id'] = df['B_id'].astype('str')\n",
    "    df['path_stops'] = df['A_id'] +\" \"+df['B_id']\n",
    "    df['path_stops'] = df['path_stops'].apply(lambda x: x.strip())\n",
    "    result_df['path_stops'] = pd.DataFrame(df.groupby(group)['path_stops'].apply(lambda x: \"%s\" % ' '.join(x).strip()))\n",
    "    # drop repeated records (A_id and B_id overlap as origin and destination nodes for subsequent trips)\n",
    "    result_df['path_stops'] = result_df['path_stops'].apply(lambda row: np.unique(row.split(' ')))\n",
    "    # write out as space separated field\n",
    "    result_df['path_stops'] = result_df['path_stops'].apply(lambda x: \"%s\" % ' '.join(x).strip())\n",
    "    \n",
    "    # Return ID field from index\n",
    "    result_df['unique_id'] = result_df.index.get_level_values(0).values\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- observed data (on-board survey [OBS])\n",
    "- route and agency lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "routes = pd.read_csv(r'../data/gtfs/agency_route_1.9.csv')\n",
    "\n",
    "# Load observed and chosenpath_links; add new field designating 'model' or 'observed'\n",
    "obs = load_df(data=obs_links_dir, unique_fields=['person_id','trip_list_id_num'], record_type='observed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill floats with integers where available, specifically for stop IDs\n",
    "obs['A_id'] = obs['A_id'].fillna(0).astype('int')\n",
    "obs['B_id'] = obs['B_id'].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- chosenpaths links output from Fast-Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chosenpath_links = load_df(data=OUTPUT_DIR + r'\\chosenpaths_links.csv', \n",
    "    unique_fields=['person_id','trip_list_id_num'], record_type='model', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For some reason there are a lot of duplicates\n",
    "# let's drop them for now\n",
    "chosenpath_links = chosenpath_links.drop_duplicates()\n",
    "\n",
    "# Get the last iteration only\n",
    "chosenpath_links = chosenpath_links[chosenpath_links['iteration'] == chosenpath_links['iteration'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add transit agency field to chosenpath_links and pathset_links, based on route_id\n",
    "chosenpath_links = add_transit_agency(df=chosenpath_links, routes=routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chosenpath_links['A_id'] = chosenpath_links['A_id'].fillna(0).astype('int')\n",
    "chosenpath_links['B_id'] = chosenpath_links['B_id'].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observed_path = produce_path_fields(obs, group=['unique_id'])\n",
    "modeled_path = produce_path_fields(chosenpath_links, group=['unique_id'])\n",
    "\n",
    "# Make sure we only evaluate the overlapping unique_id records\n",
    "observed_path = observed_path[observed_path['unique_id'].isin(modeled_path['unique_id'].values)]\n",
    "modeled_path = modeled_path[modeled_path['unique_id'].isin(observed_path['unique_id'].values)]\n",
    "\n",
    "# Ensure only single whitespaces separate comparison fields\n",
    "for field in ['path_routes','path_modes','path_agencies', 'path_components', 'path_stops']:\n",
    "    observed_path[field] = observed_path[field].apply(lambda row: ' '.join(row.split()))\n",
    "    modeled_path[field] = modeled_path[field].apply(lambda row: ' '.join(row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathset_paths = pd.read_csv(OUTPUT_DIR+ r'/pathset_paths.csv')\n",
    "pathset_paths['unique_id'] = pathset_paths['person_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build comparison fields for pathset_links\n",
    "- from the description column in pathset_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mode_list(row):\n",
    "\n",
    "    row_array = row.split(' ')\n",
    "    \n",
    "    mode_results = []\n",
    "    \n",
    "    for field in row_array:\n",
    "        if field in transit_mode_list + ['transfer','walk_access','walk_egress']:\n",
    "            mode_results.append(field)\n",
    "\n",
    "    # convert from list into space-seperated string\n",
    "    mode_results = ' '.join(mode_results).strip()\n",
    "\n",
    "    return mode_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def route_list(row):\n",
    "    \"\"\"\n",
    "    if route_only=True, return only the route id from the trip id prepended with route info\n",
    "    otherwise return full route_id_xyz, where xyz is the trip id\n",
    "    \"\"\"\n",
    "    \n",
    "    row_array = row.split(' ')\n",
    "    \n",
    "    route_results = []\n",
    "    \n",
    "    for i in xrange(len(row_array)):\n",
    "        if row_array[i] in transit_mode_list:\n",
    "            route_results.append(\"_\".join(row_array[i+1].split('_')[:-1]))     # Drop last component of field (trip ID)\n",
    "#                 route_results.append(row_array[i+1])\n",
    "            \n",
    "    route_results =  ' '.join(route_results).strip()\n",
    "    \n",
    "    return route_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stop_list(row):\n",
    "    \n",
    "    row_array = row.split(' ')\n",
    "    \n",
    "    # Get the list of transit\n",
    "    trip_with_route_list = []\n",
    "    stop_results = []\n",
    "    \n",
    "    for i in xrange(len(row_array)):\n",
    "        if row_array[i] in transit_mode_list:\n",
    "            # Save this as a field we don't want to include as a stop\n",
    "            trip_with_route_list.append(row_array[i+1])\n",
    "        if row_array[i] not in trip_with_route_list+transit_mode_list+['transfer','walk_access','walk_egress']:\n",
    "            stop_results.append(row_array[i])\n",
    "    \n",
    "    stop_results = ' '.join(stop_results).strip()\n",
    "    \n",
    "    return stop_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agency_list(row):\n",
    "    \n",
    "    agency_results = []\n",
    "    \n",
    "    # Get list of agencies associated with each transit route\n",
    "    row_array = row.split(' ')\n",
    "    \n",
    "    for route_id in row_array:\n",
    "        agency_results.append(routes[routes['route_id'] == route_id]['agency_id'].values[0])\n",
    "    \n",
    "    agency_results = ' '.join(agency_results).strip()\n",
    "    \n",
    "    return agency_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mode paths\n",
    "pathset_paths['path_modes'] = pathset_paths['description'].apply(lambda row: mode_list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Route paths\n",
    "pathset_paths['path_routes'] = pathset_paths['description'].apply(lambda row: route_list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stops\n",
    "pathset_paths['path_stops'] = pathset_paths['description'].apply(lambda row: stop_list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agencies\n",
    "pathset_paths['path_agencies'] = pathset_paths['path_routes'].apply(lambda row: agency_list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a stacked csv of observed trip links & model chosenpath_links; export for Tableau\n",
    "chosenpath_links, obs = select_common_records(chosenpath_links, obs,'person_id')\n",
    "chosenpaths_links_with_observed = append(chosenpath_links, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add fields for Tableau visualization\n",
    "- transfer from/to agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2202"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # add additional info about the transfer from and to route/agency for tableau maps\n",
    "chosenpaths_links_with_observed['transfer_from_agency'] = chosenpaths_links_with_observed['agency'].shift(1)\n",
    "chosenpaths_links_with_observed['transfer_to_agency'] = chosenpaths_links_with_observed['agency'].shift(-1)\n",
    "\n",
    "chosenpaths_links_with_observed['transfer_from_route'] = chosenpaths_links_with_observed['route_id'].shift(1)\n",
    "chosenpaths_links_with_observed['transfer_to_route'] = chosenpaths_links_with_observed['route_id'].shift(-1)\n",
    "\n",
    "len(chosenpaths_links_with_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill empty strings with 0 for origin and destination fields\n",
    "chosenpaths_links_with_observed[['A_id','B_id']] = chosenpaths_links_with_observed[['A_id','B_id']].replace('',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_id</th>\n",
       "      <th>B_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>  1395</td>\n",
       "      <td>  6513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>  6513</td>\n",
       "      <td>  6639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>  6639</td>\n",
       "      <td> 14677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 14677</td>\n",
       "      <td> 14655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 14655</td>\n",
       "      <td> 23821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A_id   B_id\n",
       "0   1395   6513\n",
       "1   6513   6639\n",
       "2   6639  14677\n",
       "3  14677  14655\n",
       "4  14655  23821"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosenpaths_links_with_observed[['A_id','B_id']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add lat/long values for stops and TAZ origins/destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add geography\n",
    "# Get distance from TAZ to stop\n",
    "chosenpaths_links_with_observed[['A_id','B_id']] = chosenpaths_links_with_observed[['A_id','B_id']].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join TAZ-centroid to stop ID distance, first for access trips (A_id as centroid, B_id as transit stop)\n",
    "chosenpaths_links_with_observed = pd.merge(chosenpaths_links_with_observed, walk_access_ft, \n",
    "                                           left_on=['A_id','B_id'],right_on=['taz','stop_id'],how='left')\n",
    "\n",
    "\n",
    "chosenpaths_links_with_observed['walk_access_dist'] = chosenpaths_links_with_observed['dist']\n",
    "chosenpaths_links_with_observed['walk_access_elevation_gain'] = chosenpaths_links_with_observed['elevation_gain']\n",
    "chosenpaths_links_with_observed['walk_access_indirectness'] = chosenpaths_links_with_observed['indirectness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now join for distance from stop to TAZ centroid (egress)\n",
    "\n",
    "# Drop existing fields from walk_access_ft for clarity\n",
    "chosenpaths_links_with_observed.drop(['taz','stop_id','dist','elevation_gain','indirectness'],axis=1,inplace=True)\n",
    "# for egress trips, A_id is stop ID and B_id is centroid, so reverse the merge fields for walk_access_ft\n",
    "chosenpaths_links_with_observed = pd.merge(chosenpaths_links_with_observed, walk_access_ft, \n",
    "                                           left_on=['A_id','B_id'],right_on=['stop_id','taz'],how='left')\n",
    "chosenpaths_links_with_observed['walk_egress_dist'] = chosenpaths_links_with_observed['dist']\n",
    "chosenpaths_links_with_observed['walk_egress_elevation_gain'] = chosenpaths_links_with_observed['elevation_gain']\n",
    "chosenpaths_links_with_observed['walk_egress_indirectness'] = chosenpaths_links_with_observed['indirectness']\n",
    "chosenpaths_links_with_observed.drop(['taz','stop_id','dist'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2202"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chosenpaths_links_with_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add lat-long where it's missing for stops, using GTFS-based network data\n",
    "# This will give lat long values associated with each A_id location\n",
    "\n",
    "# Join stop lat/lon for A_id stops\n",
    "chosenpaths_links_with_observed = pd.merge(chosenpaths_links_with_observed,stops,left_on='A_id',right_on='stop_id',how='left')\n",
    "chosenpaths_links_with_observed.rename(columns={'stop_lat':'A_lat', 'stop_lon':'A_lon'},inplace=True)\n",
    "\n",
    "chosenpaths_links_with_observed = pd.merge(chosenpaths_links_with_observed,stops,left_on='B_id',right_on='stop_id',how='left')\n",
    "chosenpaths_links_with_observed.rename(columns={'stop_lat':'B_lat','stop_lon':'B_lon'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now all stops should have lat/long\n",
    "# Next add lat and lon for TAZ fields\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For the origin TAZs\n",
    "chosenpaths_links_with_observed = pd.merge(chosenpaths_links_with_observed,taz,left_on='A_id',right_on='taz',how='left')\n",
    "chosenpaths_links_with_observed.rename(columns={'lat':'A_taz_lat','lon':'A_taz_lon'},inplace=True)\n",
    "\n",
    "# For destination TAZs\n",
    "chosenpaths_links_with_observed = pd.merge(chosenpaths_links_with_observed,taz,left_on='B_id',right_on='taz',how='left')\n",
    "chosenpaths_links_with_observed.rename(columns={'lat':'B_taz_lat','lon':'B_taz_lon'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chosenpaths_links_with_observed[['A_id','B_id','A_lat','A_lon','B_lat','B_lon','A_taz_lat','A_taz_lon','B_taz_lat','B_taz_lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill the missing lat and lon fields with taz_lat and taz_lon\n",
    "chosenpaths_links_with_observed['A_lat'].fillna(chosenpaths_links_with_observed['A_taz_lat'],inplace=True)\n",
    "chosenpaths_links_with_observed['A_lon'].fillna(chosenpaths_links_with_observed['A_taz_lon'],inplace=True)\n",
    "\n",
    "chosenpaths_links_with_observed['B_lat'].fillna(chosenpaths_links_with_observed['B_taz_lat'],inplace=True)\n",
    "chosenpaths_links_with_observed['B_lon'].fillna(chosenpaths_links_with_observed['B_taz_lon'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some access and egress distances are missing because \n",
    "# walk_access_ft doesn't include all zone to stop pairs\n",
    "# Calculate straight-line distance for all of these missing pairs\n",
    "# df = chosenpaths_links_with_observed[(chosenpaths_links_with_observed['walk_access_dist'].isnull()) &\\\n",
    "#                                      (chosenpaths_links_with_observed['mode'] == 'walk_access')]\n",
    "# df[['A_id','B_id','A_lat','A_lon','B_lat','B_lon','distance','record_type']].iloc[0]\n",
    "\n",
    "# haversine(-122.41848,37.79069,-122.39518,37.77638)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_id</th>\n",
       "      <th>B_id</th>\n",
       "      <th>A_lat</th>\n",
       "      <th>A_lon</th>\n",
       "      <th>B_lat</th>\n",
       "      <th>B_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1395.00000</td>\n",
       "      <td> 6513.00000</td>\n",
       "      <td>37.38748</td>\n",
       "      <td>-122.03915</td>\n",
       "      <td>37.39307</td>\n",
       "      <td>-122.04162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 6513.00000</td>\n",
       "      <td> 6639.00000</td>\n",
       "      <td>37.39307</td>\n",
       "      <td>-122.04162</td>\n",
       "      <td>37.39462</td>\n",
       "      <td>-122.07563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 6639.00000</td>\n",
       "      <td>14677.00000</td>\n",
       "      <td>37.39462</td>\n",
       "      <td>-122.07563</td>\n",
       "      <td>37.39406</td>\n",
       "      <td>-122.07630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14677.00000</td>\n",
       "      <td>14655.00000</td>\n",
       "      <td>37.39406</td>\n",
       "      <td>-122.07630</td>\n",
       "      <td>37.77638</td>\n",
       "      <td>-122.39518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14655.00000</td>\n",
       "      <td>23821.00000</td>\n",
       "      <td>37.77638</td>\n",
       "      <td>-122.39518</td>\n",
       "      <td>37.77709</td>\n",
       "      <td>-122.39498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         A_id        B_id    A_lat      A_lon    B_lat      B_lon\n",
       "0  1395.00000  6513.00000 37.38748 -122.03915 37.39307 -122.04162\n",
       "1  6513.00000  6639.00000 37.39307 -122.04162 37.39462 -122.07563\n",
       "2  6639.00000 14677.00000 37.39462 -122.07563 37.39406 -122.07630\n",
       "3 14677.00000 14655.00000 37.39406 -122.07630 37.77638 -122.39518\n",
       "4 14655.00000 23821.00000 37.77638 -122.39518 37.77709 -122.39498"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosenpaths_links_with_observed[['A_id','B_id','A_lat','A_lon','B_lat','B_lon']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chosenpaths_links_with_observed.to_csv(OUTPUT_DIR + '/' + 'chosenpaths_links_with_observed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Modeled v Observed\n",
    "### Produce new output file path_comparison.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine the observed and modeled path files\n",
    "df = pd.merge(observed_path, modeled_path, on='unique_id',suffixes=(\"_observed\",\"_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compare routes, modes, and agencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build list of routes used in observed and modeled trips\n",
    "df['model_path_route_list'] = df['path_routes_model'].apply(lambda x: x.split(\" \"))\n",
    "df['obs_path_route_list'] = df['path_routes_observed'].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "# Build list of modes used in observed and modeled trips\n",
    "df['model_path_mode_list'] = df['path_modes_model'].apply(lambda x: x.split(\" \"))\n",
    "df['obs_path_mode_list'] = df['path_modes_observed'].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "# Build list of transit agencies used in observed and modeled trips\n",
    "df['model_path_agencies_list'] = df['path_agencies_model'].apply(lambda x: x.split(\" \"))\n",
    "df['obs_path_agencies_list'] = df['path_agencies_observed'].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "# Build list of stops used in observed and modeled trips\n",
    "df['model_path_stops_list'] = df['path_stops_model'].apply(lambda x: x.split(\" \"))\n",
    "df['obs_path_stops_list'] = df['path_stops_observed'].apply(lambda x: x.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Isolate transit modes only, because all trips should have walk & transfer components\n",
    "    \n",
    "df['model_transit_modes'] = df['model_path_mode_list'].apply(\n",
    "    lambda row: [element for element in row if element not in non_transit_modes])\n",
    "df['obs_transit_modes'] = df['obs_path_mode_list'].apply(\n",
    "    lambda row: [element for element in row if element not in non_transit_modes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs_transit_modes</th>\n",
       "      <th>model_transit_modes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>            [commuter_rail]</td>\n",
       "      <td>         [local_bus, commuter_rail, local_bus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>               [heavy_rail]</td>\n",
       "      <td>                        [local_bus, local_bus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> [local_bus, commuter_rail]</td>\n",
       "      <td>         [commuter_rail, local_bus, local_bus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>               [heavy_rail]</td>\n",
       "      <td>                                   [local_bus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>               [heavy_rail]</td>\n",
       "      <td> [local_bus, heavy_rail, local_bus, local_bus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            obs_transit_modes                            model_transit_modes\n",
       "0             [commuter_rail]          [local_bus, commuter_rail, local_bus]\n",
       "1                [heavy_rail]                         [local_bus, local_bus]\n",
       "2  [local_bus, commuter_rail]          [commuter_rail, local_bus, local_bus]\n",
       "3                [heavy_rail]                                    [local_bus]\n",
       "4                [heavy_rail]  [local_bus, heavy_rail, local_bus, local_bus]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['obs_transit_modes','model_transit_modes']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the intersection between the chosen model/observed paths using different criteria\n",
    "# Which are in common between model and observed?\n",
    "\n",
    "# transit route IDs only\n",
    "df.apply(lambda row: all(i in row['model_path_route_list'] for i in row['obs_path_route_list']), axis=1)\n",
    "df['routes_intersection'] = [list(set(a).intersection(set(b))) for a, b in zip(df['model_path_route_list'], df['obs_path_route_list'])]\n",
    "\n",
    "# stops only\n",
    "df.apply(lambda row: all(i in row['model_path_stops_list'] for i in row['obs_path_stops_list']), axis=1)\n",
    "df['stops_intersection'] = [list(set(a).intersection(set(b))) for a, b in zip(df['model_path_stops_list'], df['obs_path_stops_list'])]\n",
    "\n",
    "# All Modes (including transfer, access/egress)\n",
    "df.apply(lambda row: all(i in row['model_path_mode_list'] for i in row['obs_path_mode_list']), axis=1)\n",
    "df['all_modes_intersection'] = [list(set(a).intersection(set(b))) for a, b in zip(df['model_path_mode_list'], df['obs_path_mode_list'])]\n",
    "\n",
    "# Transit modes only (type of vehicle taken and number of boardings)\n",
    "df.apply(lambda row: all(i in row['model_path_mode_list'] for i in row['obs_path_mode_list']), axis=1)\n",
    "df['transit_modes_intersection'] = [list(set(a).intersection(set(b))) for a, b in zip(df['model_transit_modes'], df['obs_transit_modes'])]\n",
    "\n",
    "# Agency Intersection\n",
    "df.apply(lambda row: all(i in row['model_path_agencies_list'] for i in row['obs_path_agencies_list']), axis=1)\n",
    "df['agency_intersection'] = \\\n",
    "    [list(set(a).intersection(set(b))) for a, b in zip(df['model_path_agencies_list'], \n",
    "        df['obs_path_agencies_list'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exact Match of path routes, modes, & components\n",
    "# Isolate rows (trip legs) with matching path routes\n",
    "complete_route_match = df[df['path_routes_observed'] == df['path_routes_model']]\n",
    "complete_mode_match = df[df['path_modes_observed'] == df['path_modes_model']]\n",
    "complete_agency_match = df[df['path_agencies_observed'] == df['path_agencies_model']]\n",
    "complete_stop_match = df[df['path_stops_observed'] == df['path_stops_model']]\n",
    "\n",
    "complete_route_match['complete_route_match'] = 1\n",
    "complete_mode_match['complete_mode_match'] = 1\n",
    "complete_agency_match['complete_agency_match'] = 1\n",
    "complete_stop_match['complete_stop_match'] = 1\n",
    "\n",
    "# Add new columns to the larger dataframe indicating if the row is a complete match\n",
    "df = pd.merge(df, complete_mode_match[['unique_id','complete_mode_match']], how='left', on='unique_id')\n",
    "\n",
    "\n",
    "df = pd.merge(df, complete_route_match[['unique_id','complete_route_match']], how='left', on='unique_id')\n",
    "df = pd.merge(df, complete_agency_match[['unique_id','complete_agency_match']], how='left', on='unique_id')\n",
    "df = pd.merge(df, complete_stop_match[['unique_id','complete_stop_match']], how='left', on='unique_id')\n",
    "\n",
    "for field in ['mode','route','agency', 'stop']:\n",
    "    df['complete_'+field+'_match']=  df['complete_'+field+'_match'].replace('nan',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we find the percent of trips with matching routes or partial matching routes\n",
    "\n",
    "# Join the filtered data to the original results\n",
    "df['common_route_count'] = [len(row) for row in df['routes_intersection']]\n",
    "df['common_mode_count'] = [len(row) for row in df['all_modes_intersection']]\n",
    "df['common_transit_mode_count'] = [len(row) for row in df['transit_modes_intersection']]\n",
    "df['common_agency_count'] = [len(row) for row in df['agency_intersection']]\n",
    "df['common_stop_count'] = [len(row) for row in df['stops_intersection']]\n",
    "\n",
    "# How many rows have at least one mode in common?\n",
    "df['partial_mode_match'] = [1 if row > 0 else 0 for row in df['common_mode_count']]\n",
    "df['partial_transit_mode_match'] = [1 if row > 0 else 0 for row in df['common_transit_mode_count']]\n",
    "df['partial_route_match'] = [1 if row > 0 else 0 for row in df['common_route_count']]\n",
    "df['partial_agency_match'] = [1 if row > 0 else 0 for row in df['common_agency_count']]\n",
    "df['partial_stop_match'] = [1 if row > 0 else 0 for row in df['common_stop_count']]\n",
    "\n",
    "# Export\n",
    "df.to_csv(OUTPUT_DIR + r'/path_intersection.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check if observed path is in pathset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add a field to the new_pathset that lists the pathnum\n",
    "# pathset_links['pathnum'] = pathset_links.index.get_level_values(1)\n",
    "\n",
    "# Join paths based on comparison_field, as defined in script header\n",
    "# Resulting df is merge of all observed paths that have a corresponding path in the pathset for their unique_id\n",
    "# observed paths with no corresponding path in pathset will have NaN for fields \"_pathset\" suffix\n",
    "newdf = pd.merge(observed_path, pathset_paths, how='left',\n",
    "          left_on=['unique_id',comparison_field],right_on=['unique_id',comparison_field], suffixes=['_obs','_pathset'])\n",
    "\n",
    "# Join this data with the pathset path file to get pf_probability associated with the observed path\n",
    "# newdf = pd.merge(df, pathset_paths[['unique_id','pathnum','pf_probability']], \n",
    "#                  left_on=['unique_id','pathnum'], right_on=['unique_id','pathnum'],\n",
    "#                  how='left')\n",
    "\n",
    "# Fields with NaN marked as no match since no matching path was found in pathset\n",
    "newdf['probability'] = newdf['pf_probability'].fillna('no_match')\n",
    "\n",
    "# Grab the highest and lowest probabilities from pathset paths\n",
    "# want to test that the observed path has a reasonably high probability\n",
    "max_prob = newdf.groupby('unique_id').max()['probability']\n",
    "min_prob = newdf.groupby('unique_id').min()['probability']\n",
    "\n",
    "# Reshape those results and export to dataframe\n",
    "prob_export = pd.DataFrame([max_prob,min_prob]).T\n",
    "prob_export.columns = ['max_prob','min_prob']\n",
    "\n",
    "\n",
    "# Reformat at binary to indicate whether a path was found in the pathset\n",
    "prob_export['path_exists'] = prob_export['max_prob'].apply(lambda row_value: 0 if row_value == 'no_match' else 1)\n",
    "prob_export.to_csv('temp_prob_export.csv')\n",
    "\n",
    "# Create a variabale to indicate if the max probability of the observed path\n",
    "# is over a given threshold, as defined at top of script\n",
    "try:\n",
    "    prob_export.ix[prob_export['max_prob'] >= threshold, 'above_threshold'] = 1\n",
    "    prob_export.ix[prob_export['max_prob'] < threshold, 'above_threshold'] = 0\n",
    "    prob_export.ix[prob_export['max_prob'] == 'no_match', 'above_threshold'] = -1\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# export the results probabilities, path_existence, threshold data,\n",
    "# also add the modeled and observed (chosen) path characteristics\n",
    "prob_export['unique_id'] = prob_export.index\n",
    "\n",
    "tempdf = pd.merge(observed_path,modeled_path,on='unique_id',suffixes=['_obs','_model'],how='left')\n",
    "export_df = pd.merge(prob_export,tempdf,on='unique_id',how='left')\n",
    "\n",
    "export_df['person_id'] = export_df['unique_id'].apply(lambda row: row.split(\"_\")[0])\n",
    "export_df['trip_list_id_num'] = export_df['unique_id'].apply(lambda row: row.split(\"_\")[-1])\n",
    "\n",
    "export_df.to_csv(OUTPUT_DIR + '\\path_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012658227848101266"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_df['path_exists'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_exists       1.00000\n",
       "above_threshold   1.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_df[export_df['max_prob'] != 'no_match'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['complete_route_match'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['complete_stop_match'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012658227848101266"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['complete_agency_match'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Add time fields\n",
    "- OBS data has no time fields, but we still want to query by time of day so copy FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
